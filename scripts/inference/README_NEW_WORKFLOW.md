# ðŸš€ Quick Start: BIT* vs MPD Comparison

**New streamlined workflow for running comprehensive comparisons!**

## Run Complete Comparison (Recommended)

Use this **single command** to run both BIT* and MPD on multiple problems:

```bash
python run_complete_comparison.py --n-problems 10
```

This will:
- âœ… Generate 10 random start/goal pairs
- âœ… Run BIT* on each for **600 seconds** with **per-second tracking**
- âœ… Run MPD on each with **NO visualization** (much faster)
- âœ… Collect comprehensive metrics for both algorithms
- âœ… Save all data in multiple formats

**ðŸ“– See [COMPLETE_COMPARISON_GUIDE.md](COMPLETE_COMPARISON_GUIDE.md) for full documentation**

## Analyze Results

After running the comparison, analyze and visualize the results:

```bash
python analyze_comparison_results.py
```

This generates:
- Comprehensive statistics printed to console
- Path length comparison plots
- BIT* optimization over time plots
- MPD collision rate analysis
- Timing analysis plots

## Data Collected

### BIT* Metrics (Per Second)
- Path length, smoothness, mean jerk
- Tree size (number of vertices)
- **Number of batches sampled**
- Has solution (boolean)

### MPD Metrics (Per Problem)
- **Collision frequency** (rate of trajectories in collision)
- **Mean collision-free path length**
- **Best collision-free path length**
- Smoothness
- Inference time

### Comparison Metrics
- Time when BIT* matched/beat MPD
- Time when BIT* found first solution
- Success rates for both algorithms

## Output Files

All saved in `multi_run_results/`:

```
multi_run_results/
â”œâ”€â”€ problem_set.pt                      # Start/goal pairs (reusable)
â”œâ”€â”€ problem_set.txt                     # Human-readable problem list
â”œâ”€â”€ bitstar_result_000.pt               # BIT* result for problem 0 (with per-second data)
â”œâ”€â”€ bitstar_result_001.pt               # BIT* result for problem 1
â”œâ”€â”€ ...
â”œâ”€â”€ mpd_results/
â”‚   â”œâ”€â”€ mpd_result_000.pt              # MPD result for problem 0
â”‚   â”œâ”€â”€ mpd_result_001.pt              # MPD result for problem 1
â”‚   â””â”€â”€ ...
â”œâ”€â”€ complete_aggregated_results.pt      # All results combined (Python)
â”œâ”€â”€ complete_statistics.yaml            # Summary statistics (YAML)
â”œâ”€â”€ comparison_data.json                # Per-problem comparison (JSON)
â””â”€â”€ [plots generated by analyze script]
```

## Quick Examples

### Run 5 problems with custom settings
```bash
python run_complete_comparison.py \
    --n-problems 5 \
    --bitstar-time 300 \
    --mpd-samples 128
```

### High-detail BIT* tracking (0.5s intervals)
```bash
python run_complete_comparison.py \
    --n-problems 10 \
    --interval 0.5
```

### Use custom output directory
```bash
python run_complete_comparison.py \
    --n-problems 10 \
    --output-dir my_experiment
```

### Analyze specific results
```bash
python analyze_comparison_results.py \
    --results my_experiment/complete_aggregated_results.pt \
    --output-dir my_plots
```

## What's Different from Old Workflow?

**Old workflow** ([MULTI_RUN_GUIDE.md](MULTI_RUN_GUIDE.md)):
1. Run `run_multiple_comparisons.py` (BIT* only)
2. Run `run_inference_on_problems.py` (MPD only)
3. Run `run_multiple_comparisons.py --load-mpd` (combine)

**New workflow** (this!):
1. Run `run_complete_comparison.py` âœ… **Done!**

Plus:
- âœ… More detailed metrics (collision frequency, per-second batch tracking, etc.)
- âœ… MPD runs with visualization disabled (much faster)
- âœ… Better data formats (JSON, YAML, PyTorch)
- âœ… Built-in analysis script with plots

## Python Analysis Example

```python
import torch
import matplotlib.pyplot as plt

# Load results
results = torch.load('multi_run_results/complete_aggregated_results.pt')

# Print statistics
print(f"BIT* success rate: {results['bitstar_stats']['success_rate']*100:.1f}%")
print(f"MPD success rate: {results['mpd_stats']['success_rate']*100:.1f}%")
print(f"BIT* beats MPD: {results['comparison_stats']['bitstar_beats_mpd_rate']*100:.1f}%")

# Plot BIT* optimization for problem 0
problem_0 = results['bitstar_results'][0]
if problem_0['success']:
    times = [m['time'] for m in problem_0['interval_metrics'] if m['has_solution']]
    lengths = [m['path_length'] for m in problem_0['interval_metrics'] if m['has_solution']]
    batches = [m['num_batches'] for m in problem_0['interval_metrics'] if m['has_solution']]

    plt.figure(figsize=(10, 6))
    plt.plot(times, lengths, 'b-', linewidth=2)
    plt.xlabel('Time (s)')
    plt.ylabel('Path Length')
    plt.title('BIT* Optimization Over Time')
    plt.grid(True)
    plt.savefig('bitstar_optimization.png')
```

## Command-Line Options

```bash
python run_complete_comparison.py --help
```

Key options:
- `--n-problems N`: Number of problems (default: 10)
- `--bitstar-time T`: BIT* time limit in seconds (default: 600)
- `--interval I`: BIT* tracking interval (default: 1.0)
- `--mpd-samples N`: MPD trajectory samples (default: 64)
- `--output-dir DIR`: Output directory (default: multi_run_results)

## Need Help?

- **Complete documentation**: [COMPLETE_COMPARISON_GUIDE.md](COMPLETE_COMPARISON_GUIDE.md)
- **Old workflow**: [MULTI_RUN_GUIDE.md](MULTI_RUN_GUIDE.md)
- **BIT* baseline info**: [COMPARISON_PYTHON_VS_OMPL_BITSTAR.md](COMPARISON_PYTHON_VS_OMPL_BITSTAR.md)

## Tips

1. **Start small**: Test with 2-3 problems first
2. **BIT* time**: 600s gives good results, reduce to 300s for faster tests
3. **Reuse problems**: Keep the same `problem_set.pt` to compare different settings
4. **GPU memory**: If OOM, reduce `--mpd-samples` to 32

---

**Ready to run?**

```bash
python run_complete_comparison.py --n-problems 10
python analyze_comparison_results.py
```

Enjoy! ðŸŽ‰
