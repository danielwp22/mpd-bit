# CSV Output Guide

## Overview

The `run_complete_comparison.py` script now automatically generates comprehensive CSV files with all comparison data. You can also export CSVs from existing results using `export_results_to_csv.py`.

## CSV Files Generated

### 1. `bitstar_timeseries.csv`
**Second-by-second BIT* optimization data**

Contains one row per time interval (typically 1 second) for each problem, tracking BIT*'s optimization progress over time.

**Columns:**
- `problem_idx` - Problem ID (0, 1, 2, ...)
- `time` - Elapsed time in seconds
- `iteration` - BIT* iteration number
- `path_length` - Current best path length at this time
- `smoothness` - Current best path smoothness at this time
- `mean_jerk` - Current best mean jerk at this time
- `num_vertices` - Number of vertices in search tree
- `num_samples` - Number of samples in tree
- `num_batches` - Number of sampling batches completed
- `has_solution` - Whether a solution exists at this time (True/False)
- `start_state` - Starting configuration (comma-separated joint angles)
- `goal_state` - Goal configuration (comma-separated joint angles)

**Use cases:**
- Plot BIT* convergence over time
- Analyze optimization trajectory
- Compare convergence speed across problems
- Create time-series visualizations

**Example:**
```
problem_idx,time,path_length,smoothness,num_vertices,has_solution
0,1.0,5.234,0.123,145,True
0,2.0,4.891,0.098,287,True
0,3.0,4.623,0.087,421,True
```

---

### 2. `mpd_results.csv`
**MPD results per problem**

Contains one row per problem with MPD's performance metrics and statistics across all generated samples.

**Columns:**
- `problem_idx` - Problem ID
- `success` - Whether MPD completed successfully
- `inference_time` - Time taken for inference (seconds)
- `n_samples` - Number of trajectory samples generated (typically 64)
- `n_diffusion_steps` - Number of diffusion steps used
- `best_path_length` - Shortest path length across all samples
- `best_smoothness` - Smoothness of best trajectory
- `n_collision_free` - Number of collision-free trajectories
- `n_total_samples` - Total samples generated
- `collision_rate` - Fraction of trajectories in collision (0.0 to 1.0)
- `mean_collision_free_path_length` - Average path length of collision-free paths
- `best_collision_free_path_length` - Best collision-free path length
- `path_length_mean` - Mean path length across all samples
- `path_length_std` - Standard deviation of path lengths
- `smoothness_mean` - Mean smoothness across all samples
- `smoothness_std` - Standard deviation of smoothness
- `start_state` - Starting configuration
- `goal_state` - Goal configuration

**Use cases:**
- Compare MPD performance across problems
- Analyze collision rates
- Study path length distributions
- Evaluate consistency (std deviation)

**Example:**
```
problem_idx,best_path_length,collision_rate,n_collision_free,mean_collision_free_path_length
0,4.234,0.125,56,4.567
1,3.891,0.031,62,4.023
```

---

### 3. `mpd_all_samples.csv`
**Individual MPD sample data**

Contains one row per sample per problem, showing metrics for each of the ~64 trajectories generated by MPD for each problem.

**Columns:**
- `problem_idx` - Problem ID
- `sample_idx` - Sample index (0 to 63 typically)
- `path_length` - Path length of this sample
- `smoothness` - Smoothness of this sample
- `is_collision_free` - Whether this sample is collision-free
- `start_state` - Starting configuration
- `goal_state` - Goal configuration

**Use cases:**
- Analyze distribution of MPD outputs
- Study correlation between path length and collision
- Create histograms of path quality
- Identify outliers

**Example:**
```
problem_idx,sample_idx,path_length,smoothness,is_collision_free
0,0,4.567,0.092,True
0,1,5.123,0.145,False
0,2,4.234,0.078,True
...
1,0,3.891,0.067,True
```

---

### 4. `comparison_summary.csv`
**Head-to-head comparison per problem**

Contains one row per problem comparing BIT* and MPD performance.

**Columns:**
- `problem_idx` - Problem ID
- `bitstar_success` - Whether BIT* succeeded
- `mpd_success` - Whether MPD succeeded
- `mpd_target_length` - Target path length from MPD (best collision-free)
- `bitstar_final_length` - BIT*'s final path length
- `time_to_first_solution` - Time for BIT* to find first solution (seconds)
- `time_to_match_mpd` - Time for BIT* to match/beat MPD's quality (seconds, or "N/A")
- `bitstar_beats_mpd` - Whether BIT* final solution beats/matches MPD

**Use cases:**
- Determine which algorithm performed better per problem
- Analyze convergence times
- Create win/loss comparisons
- Study time-to-quality metrics

**Example:**
```
problem_idx,bitstar_final_length,mpd_target_length,time_to_match_mpd,bitstar_beats_mpd
0,4.123,4.234,45.2,True
1,5.234,4.891,N/A,False
```

---

## Usage

### Automatic Export (Recommended)

When you run `run_complete_comparison.py`, CSV files are automatically generated:

```bash
python run_complete_comparison.py --n-problems 10
```

CSV files will be in `multi_run_results/`:
- `bitstar_timeseries.csv`
- `mpd_results.csv`
- `mpd_all_samples.csv`
- `comparison_summary.csv`

### Manual Export from Existing Results

If you've already run the comparison and want to export CSVs:

```bash
# Export from default directory
python export_results_to_csv.py

# Export from custom directory
python export_results_to_csv.py --input-dir my_results

# Export with custom filename prefix
python export_results_to_csv.py --input-dir my_results --output-name exp1
```

---

## Data Analysis Examples

### Python/Pandas

```python
import pandas as pd
import matplotlib.pyplot as plt

# Load BIT* time-series data
bitstar_ts = pd.read_csv('multi_run_results/bitstar_timeseries.csv')

# Plot convergence for problem 0
problem_0 = bitstar_ts[bitstar_ts['problem_idx'] == 0]
plt.plot(problem_0['time'], problem_0['path_length'])
plt.xlabel('Time (s)')
plt.ylabel('Path Length')
plt.title('BIT* Convergence - Problem 0')
plt.show()

# Load MPD results
mpd_results = pd.read_csv('multi_run_results/mpd_results.csv')
print(f"Average collision rate: {mpd_results['collision_rate'].mean():.2%}")

# Load comparison summary
comparison = pd.read_csv('multi_run_results/comparison_summary.csv')
win_rate = comparison['bitstar_beats_mpd'].mean()
print(f"BIT* win rate: {win_rate:.2%}")
```

### Excel

1. Open any CSV file in Excel
2. Use built-in charts and pivot tables
3. Filter by `problem_idx` to analyze specific problems
4. Create comparison charts between algorithms

### R

```r
library(tidyverse)

# Load and analyze BIT* time-series
bitstar <- read_csv('multi_run_results/bitstar_timeseries.csv')

# Plot all problems' convergence
ggplot(bitstar, aes(x=time, y=path_length, color=factor(problem_idx))) +
  geom_line() +
  labs(title="BIT* Convergence Across All Problems")
```

---

## File Sizes

Approximate CSV file sizes for reference:

- **10 problems, 600 seconds each:**
  - `bitstar_timeseries.csv`: ~1-2 MB (6,000 rows)
  - `mpd_results.csv`: ~10 KB (10 rows)
  - `mpd_all_samples.csv`: ~50-100 KB (640 rows)
  - `comparison_summary.csv`: ~5 KB (10 rows)

- **100 problems, 600 seconds each:**
  - `bitstar_timeseries.csv`: ~10-20 MB (60,000 rows)
  - `mpd_results.csv`: ~100 KB (100 rows)
  - `mpd_all_samples.csv`: ~500 KB - 1 MB (6,400 rows)
  - `comparison_summary.csv`: ~50 KB (100 rows)

---

## Notes

1. **Start/Goal States**: Stored as comma-separated values in CSV. For Panda robot, these are 7-dimensional joint configurations.

2. **Infinite Values**: Failed or invalid metrics are stored as `inf` in the CSV.

3. **N/A Values**: When BIT* doesn't match MPD's quality, `time_to_match_mpd` is "N/A".

4. **Memory**: For large-scale experiments (1000+ problems), consider processing CSVs in chunks using pandas `chunksize` parameter.

5. **Precision**: Floating-point values are written with Python's default precision. For custom precision, modify the export functions.
